{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import unidecode\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_value = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(input_list, n):    \n",
    "    return zip(*[input_list[i:] for i in range(n)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllPossibleWords(file_path): \n",
    "    possible_words=[]\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            for word in line.split():\n",
    "                possible_words.append(word)     \n",
    "    possible_words = map(lambda x : unidecode.unidecode(x),possible_words)\n",
    "    for c in ['`',\"[\",\"]\",\"(\",\")\",\"\\\"\",\".\",\"''\",\",\"]:\n",
    "        possible_words = map(lambda x: x.replace(c,\"\"),possible_words)\n",
    "        \n",
    "    possible_words = map(lambda x: x.replace('&','And'),possible_words)\n",
    "    #possible_words = map(lambda x: x.replace('of','Of'),possible_words)\n",
    "    possible_words=filter(lambda x:len(x)>0,possible_words)    \n",
    "    \n",
    "    return possible_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListFromFiles(filename):\n",
    "    file_path = os.path.join(os.getcwd(),filename)\n",
    "    info_list = []\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            for word in line.split():\n",
    "                info_list.append(word)\n",
    "    return info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToCsv(processed_list):\n",
    "    with open(\"output.csv\",'wb') as resultFile:\n",
    "        wr = csv.writer(resultFile, dialect='excel')\n",
    "        for row in processed_list:\n",
    "            wr.writerows(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(possible_words,doc_id):    \n",
    "    suffix_list = getListFromFiles(\"company-suffixes.txt\")\n",
    "    prefix_list = getListFromFiles(\"company-prefixes.txt\")    \n",
    "    common_words = getListFromFiles(\"common-words.txt\")\n",
    "    preprocessed_list =[]    \n",
    "    marked_list=[]\n",
    "    word_number=len(possible_words)\n",
    "    for i in range(5,0,-1):\n",
    "        word_count=0\n",
    "        ngram_list = get_ngrams(possible_words,i)\n",
    "        for ngram in ngram_list:            \n",
    "            #print ngram   \n",
    "            \n",
    "            #if markup is both at the beginning or at the end of the word group accept and label as 1\n",
    "            if(len(ngram)>1 and '<markup>' in ngram[0] and '</markup>' in ngram[len(ngram)-1] and\n",
    "              all((word[0].isupper() or \"markup\" in word) for word in ngram) and \n",
    "              [\"<markup>\" in word for word in ngram].count(True)==1):\n",
    "                preprocessed_list.append([' '.join(ngram),doc_id,word_count,word_count+i-1,1])\n",
    "            \n",
    "            #get all instances of format <markup>Microsoft</markup>\n",
    "            if(len(ngram)==1 and '</markup>' in ngram[0] and '<markup>' in ngram[0]):\n",
    "                preprocessed_list.append([' '.join(ngram),doc_id,word_count,word_count+i-1,1])            \n",
    "                    \n",
    "            else:\n",
    "                #prune away all n grams with non uppercase first character\n",
    "                if(all(word[0].isupper() and \"markup\" not in word for word in ngram)):       \n",
    "                    \n",
    "                    # prune common words\n",
    "                    if(any(word not in common_words for word in ngram)):                        \n",
    "                        preprocessed_list.append([' '.join(ngram),doc_id,word_count,word_count+i-1,0])\n",
    "\n",
    "                    #prune unigrams \n",
    "                    elif(i==1 and word_number> word_count+1):                        \n",
    "                        \n",
    "                        #add the unigram to the list if the next word begins with lower case\n",
    "                        if(possible_words[word_count+1][0].islower()):\n",
    "                            preprocessed_list.append([' '.join(ngram),doc_id,word_count,word_count+i-1,0])  \n",
    "            word_count = word_count+1\n",
    "    return preprocessed_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = os.path.dirname(os.getcwd())\n",
    "data_directory = os.path.join(base_directory,\"Dataset\",\"Rahul\",\"Final\")\n",
    "file_list = os.listdir(data_directory)\n",
    "possible_words=[]\n",
    "processed_list=[]\n",
    "count=0\n",
    "for file in file_list:\n",
    "    doc_id = file[0:3]\n",
    "    #print(doc_id)\n",
    "    #if(int(doc_id)!=95):\n",
    "    #    continue    \n",
    "    possible_words = getAllPossibleWords(os.path.join(data_directory,file))     \n",
    "    \n",
    "    #start from here .. processessed list has the structure[[word,doc_id,word_count_start,word_count_end,label]]\n",
    "    processed_list.append(preprocessing(possible_words,doc_id))\n",
    "    writeToCsv(processed_list)   \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[\"National Football League's American Conference\", '000', 70, 74, 0],\n",
       "  [\"National Football League's American\", '000', 70, 73, 0],\n",
       "  [\"Football League's American Conference\", '000', 71, 74, 0],\n",
       "  ['Nielsen Media Research', '000', 39, 41, 0],\n",
       "  [\"National Football League's\", '000', 70, 72, 0],\n",
       "  [\"Football League's American\", '000', 71, 73, 0],\n",
       "  [\"League's American Conference\", '000', 72, 74, 0],\n",
       "  ['<markup>Katz Television Group</markup>', '000', 142, 144, 1],\n",
       "  ['New York A', '000', 146, 148, 0],\n",
       "  ['American Idol', '000', 0, 1, 0],\n",
       "  ['Nielsen Media', '000', 39, 40, 0],\n",
       "  ['Media Research', '000', 40, 41, 0],\n",
       "  ['National Football', '000', 70, 71, 0],\n",
       "  [\"Football League's\", '000', 71, 72, 0],\n",
       "  [\"League's American\", '000', 72, 73, 0],\n",
       "  ['American Conference', '000', 73, 74, 0],\n",
       "  ['Simon Cowell', '000', 113, 114, 0],\n",
       "  ['Randy Jackson', '000', 116, 117, 0],\n",
       "  ['Bill Carroll', '000', 132, 133, 0],\n",
       "  ['New York', '000', 146, 147, 0],\n",
       "  ['York A', '000', 147, 148, 0],\n",
       "  ['American', '000', 0, 0, 0],\n",
       "  ['Idol', '000', 1, 1, 0],\n",
       "  ['Idol', '000', 1, 1, 0],\n",
       "  ['With', '000', 29, 29, 0],\n",
       "  ['Nielsen', '000', 39, 39, 0],\n",
       "  ['Media', '000', 40, 40, 0],\n",
       "  ['Research', '000', 41, 41, 0],\n",
       "  ['Research', '000', 41, 41, 0],\n",
       "  ['Idol', '000', 44, 44, 0],\n",
       "  ['Idol', '000', 44, 44, 0],\n",
       "  ['Idol', '000', 49, 49, 0],\n",
       "  ['Idol', '000', 49, 49, 0],\n",
       "  ['Fox', '000', 57, 57, 0],\n",
       "  ['Fox', '000', 57, 57, 0],\n",
       "  ['National', '000', 70, 70, 0],\n",
       "  ['Football', '000', 71, 71, 0],\n",
       "  [\"League's\", '000', 72, 72, 0],\n",
       "  ['American', '000', 73, 73, 0],\n",
       "  ['Conference', '000', 74, 74, 0],\n",
       "  ['Conference', '000', 74, 74, 0],\n",
       "  ['CBS', '000', 78, 78, 0],\n",
       "  ['CBS', '000', 78, 78, 0],\n",
       "  ['Sunday', '000', 80, 80, 0],\n",
       "  ['Sunday', '000', 80, 80, 0],\n",
       "  ['The', '000', 85, 85, 0],\n",
       "  ['Idol', '000', 88, 88, 0],\n",
       "  ['Idol', '000', 88, 88, 0],\n",
       "  ['They', '000', 104, 104, 0],\n",
       "  ['Simon', '000', 113, 113, 0],\n",
       "  ['Cowell', '000', 114, 114, 0],\n",
       "  ['Cowell', '000', 114, 114, 0],\n",
       "  ['Randy', '000', 116, 116, 0],\n",
       "  ['Jackson', '000', 117, 117, 0],\n",
       "  ['Jackson', '000', 117, 117, 0],\n",
       "  ['Controversy', '000', 128, 128, 0],\n",
       "  ['Controversy', '000', 128, 128, 0],\n",
       "  ['Bill', '000', 132, 132, 0],\n",
       "  ['Carroll', '000', 133, 133, 0],\n",
       "  ['Carroll', '000', 133, 133, 0],\n",
       "  ['Television', '000', 143, 143, 0],\n",
       "  ['New', '000', 146, 146, 0],\n",
       "  ['York', '000', 147, 147, 0],\n",
       "  ['A', '000', 148, 148, 0],\n",
       "  ['The', '000', 166, 166, 0],\n",
       "  ['Idol', '000', 169, 169, 0],\n",
       "  ['Idol', '000', 169, 169, 0],\n",
       "  ['Carroll', '000', 184, 184, 0],\n",
       "  ['Carroll', '000', 184, 184, 0]]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallo\n"
     ]
    }
   ],
   "source": [
    "ngram = ('<markup>Gazprom', 'Lukoil</markup>')\n",
    "if(len(ngram)>1 and '<markup>' in ngram[0] and '</markup>' in ngram[len(ngram)-1] and\n",
    "              all((word[0].isupper() or \"markup\" in word) for word in ngram) and \n",
    "              [\"<markup>\" in word for word in ngram].count(True)==1):\n",
    "    print \"hallo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"markup\" in word for word in ngram].count(True)==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
