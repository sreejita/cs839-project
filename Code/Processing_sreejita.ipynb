{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_value = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(input_list, n):    \n",
    "    return zip(*[input_list[i:] for i in range(n)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllPossibleWords(file_path): \n",
    "    possible_words=[]\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            for word in line.split():\n",
    "                possible_words.append(word)     \n",
    "   # possible_words = map(lambda x : unidecode.unidecode(x),possible_words)\n",
    "    for c in ['`',\"[\",\"]\",\"(\",\")\",\"\\\"\",\".\",\"''\",\",\"]:\n",
    "        possible_words = map(lambda x: x.replace(c,\"\"),possible_words)\n",
    "        \n",
    "    possible_words = map(lambda x: x.replace('&','And'),possible_words)\n",
    "    #possible_words = map(lambda x: x.replace('of','Of'),possible_words)\n",
    "    possible_words=filter(lambda x:len(x)>0,possible_words)    \n",
    "    \n",
    "    return possible_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListFromFiles(filename):\n",
    "    file_path = os.path.join(os.getcwd(),filename)\n",
    "    info_list = []\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            for word in line.split():\n",
    "                info_list.append(word)\n",
    "    return info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToCsv(processed_list):\n",
    "    with open(\"output-sreejita.csv\",'wb') as resultFile:\n",
    "        wr = csv.writer(resultFile, dialect='excel')\n",
    "        #for row in processed_list:\n",
    "        wr.writerows(processed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(possible_words,doc_id):    \n",
    "    suffix_list = getListFromFiles(\"company-suffixes.txt\")\n",
    "    prefix_list = getListFromFiles(\"company-prefixes.txt\")    \n",
    "    common_words = getListFromFiles(\"common-words.txt\")\n",
    "    preprocessed_list =[]    \n",
    "    marked_list=[]\n",
    "    word_number=len(possible_words)\n",
    "    for i in range(5,0,-1):\n",
    "        word_count=0\n",
    "        ngram_list = get_ngrams(possible_words,i)\n",
    "        for ngram in ngram_list:            \n",
    "            #print ngram   \n",
    "            \n",
    "            #if markup is both at the beginning or at the end of the word group accept and label as 1\n",
    "            if(len(ngram)>1 and '<markup>' in ngram[0] and '</markup>' in ngram[len(ngram)-1] and\n",
    "              all((word[0].isupper() or \"markup\" in word) for word in ngram) and \n",
    "              [\"<markup>\" in word for word in ngram].count(True)==1): \n",
    "                company_tuple = ngram\n",
    "                for string in [\"<markup>\",\"</markup>\"]:\n",
    "                    company_tuple = map(lambda x : x.replace(string,\"\"),company_tuple)                \n",
    "                preprocessed_list.append([' '.join(company_tuple),doc_id,word_count,word_count+i-1,1])\n",
    "            \n",
    "            #get all instances of format <markup>Microsoft</markup>\n",
    "            elif(len(ngram)==1 and '</markup>' in ngram[0] and '<markup>' in ngram[0]):\n",
    "                \n",
    "                company_tuple = ngram\n",
    "                for string in [\"<markup>\",\"</markup>\"]:\n",
    "                    company_tuple = map(lambda x : x.replace(string,\"\"),company_tuple) \n",
    "                \n",
    "                preprocessed_list.append([' '.join(company_tuple),doc_id,word_count,word_count+i-1,1])            \n",
    "                    \n",
    "            else:\n",
    "                #prune away all n grams with non uppercase first character\n",
    "                if(all(word[0].isupper() and \"markup\" not in word for word in ngram)):  \n",
    "                    \n",
    "                    #prune unigrams \n",
    "                    if(i==1):                        \n",
    "                        if(ngram[0] not in suffix_list):\n",
    "                            if(word_number> word_count+1 and ngram[0] not in common_words):\n",
    "                                #add the unigram to the list if the next word begins with lower case\n",
    "                                if(possible_words[word_count+1][0].islower()):\n",
    "                                    preprocessed_list.append([' '.join(ngram),doc_id,word_count,word_count+i-1,0])\n",
    "                    \n",
    "                    #prune common words\n",
    "                    elif(any(word not in common_words for word in ngram)):                        \n",
    "                        preprocessed_list.append([' '.join(ngram),doc_id,word_count,word_count+i-1,0])                   \n",
    "\n",
    "                    \n",
    "                     \n",
    "            word_count = word_count+1\n",
    "    return preprocessed_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFeatures(filename, processed_list,possible_words):\n",
    "    \n",
    "    company_suffixes = getListFromFiles(\"company-suffixes.txt\")\n",
    "    company_prefixes = getListFromFiles(\"company-prefixes.txt\")    \n",
    "    company_list=[]\n",
    "    final_processed_list = []\n",
    "    splittedFilename = filename.split(\"-\")\n",
    "    i=0\n",
    "    #print \"possible_words- \"  \n",
    "    #print possible_words\n",
    "    for company in processed_list:\n",
    "        i+=1\n",
    "        #Feature 1 : name has any prefix or suffix of a company \n",
    "        #if company name has any one of the suffixes or prefixes set it to true \n",
    "        hasCompanyid=0        \n",
    "        if(company[0].split()[-1] in company_suffixes or company[0].split()[0] in company_prefixes):\n",
    "            hasCompanyid = 1\n",
    "            company_list.append(company[0])            \n",
    "        #company.append(hasCompanyid)\n",
    "        \n",
    "        \n",
    "        #has Company name at the beginning ?\n",
    "        #Feature 2: name occurs at the beginning of a company name\n",
    "        hasCompanyNameFirst = 0\n",
    "        l1= [x for x in company[0].split() if x not in company_prefixes]\n",
    "        l2=[]\n",
    "        for comp in company_list:    \n",
    "            if(comp.split()[0] in company_prefixes):\n",
    "                l2.append(\" \".join(comp.split()[1:]))\n",
    "            else:\n",
    "                l2.append(comp)\n",
    "        if(any(word in l2 for word in l1)):\n",
    "            hasCompanyNameFirst = 1\n",
    "        elif(any(company[0].split()[0]== word.split()[0] for word in company_list)):\n",
    "            hasCompanyNameFirst = 1 \n",
    "        \n",
    "            \n",
    "        \n",
    "        #has Company name as a substring ?\n",
    "        #Feature 3: name is a substring in the company name\n",
    "        hasNameSubstring=0\n",
    "        for comp in company_list:\n",
    "            l1 = company[0].split()\n",
    "            l2 = comp.split()\n",
    "            if(len(set(l1)&set(l2)) > 0 ):\n",
    "                hasNameSubstring=1       \n",
    "       \n",
    "         \n",
    "        \n",
    "        #Feature 4: name in filename\n",
    "        isInFilename = 0\n",
    "        splittedInstance = company[0].split()\n",
    "        for sp in splittedInstance:\n",
    "            if sp.lower() in splittedFilename:\n",
    "                isInFilename = 1 \n",
    "        \n",
    "        \n",
    "        #Feature 5-y - Check neighbouring words - check 3 words before and 3 words after\n",
    "        starting_loc = company[2]\n",
    "        ending_loc = company[3]\n",
    "        words_before_starting_loc = []\n",
    "        words_after_ending_loc = []\n",
    "        j= starting_loc-1\n",
    "        count = 1\n",
    "        while j >= 0 and count <= 3:\n",
    "            words_before_starting_loc.append(possible_words[company[2]-count])\n",
    "            j-=1\n",
    "            count+=1\n",
    "        \n",
    "        j = ending_loc+1\n",
    "        count = 1\n",
    "        while j < len(possible_words) and count <= 3:\n",
    "            words_after_ending_loc.append(possible_words[company[3]+count])\n",
    "            j+=1\n",
    "            count+=1\n",
    "        \n",
    "        #Feature 5 - Check for word \"based\" before the company\n",
    "        has_based = 0\n",
    "        for word in words_before_starting_loc:\n",
    "            if \"based\" in word.lower():\n",
    "                has_based = 1\n",
    "        \n",
    "        #Feature 6 \"'s\" is after the company\n",
    "        has_apostrophe_s = 0\n",
    "        for word in words_after_ending_loc:\n",
    "            if \"'s\" in word.lower():\n",
    "                has_apostrophe_s = 1\n",
    "        '''\n",
    "        #Feature 7 \",\" is after/before the company\n",
    "        has_comma = 0\n",
    "        for word in words_after_ending_loc:\n",
    "            if \",\" in word.lower():\n",
    "                has_comma = 1\n",
    "        \n",
    "        for word in words_before_starting_loc:\n",
    "            if \",\" in word.lower():\n",
    "                has_comma = 1\n",
    "        '''\n",
    "        #Feature \"shares before or after the company\n",
    "        has_shares = 0\n",
    "        for word in words_after_ending_loc:\n",
    "            if \"shares\" in word.lower():\n",
    "                has_shares = 1\n",
    "              \n",
    "        for word in words_before_starting_loc:\n",
    "            if \"shares\" in word.lower():\n",
    "                has_shares = 1 \n",
    "                \n",
    "        #Feature 8 designations before/after company\n",
    "        desig = open('designations.txt').read().split('\\n')\n",
    "        desig_lower = [x.lower() for x in desig]\n",
    "        has_desig = 0\n",
    "        for word in words_after_ending_loc:\n",
    "            if word.lower() in desig_lower:\n",
    "                has_desig = 1\n",
    "              \n",
    "        for word in words_before_starting_loc:\n",
    "            if word.lower() in desig_lower:\n",
    "                has_desig = 1 \n",
    "         \n",
    "        #Feature 9 \"said/told\" in the before company\n",
    "        has_said = 0\n",
    "        for word in words_before_starting_loc:\n",
    "            if \"said\" in word.lower() or \"told\" in word.lower():\n",
    "                has_said = 1\n",
    "        \n",
    "        '''\n",
    "        #Feature 10 Prune away Examples if previous/next word starts with a Capital\n",
    "        surrounded_capital = 0\n",
    "        if len(words_before_starting_loc) > 0:\n",
    "            prev_word = words_before_starting_loc[0]\n",
    "            if len(prev_word) > 0 and prev_word[0].isupper():\n",
    "                surrounded_capital = 1\n",
    "        if len( words_after_ending_loc) > 0:\n",
    "            next_word = words_after_ending_loc[0]\n",
    "            if len(next_word) > 0 and next_word[0].isupper():\n",
    "                surrounded_capital = 1\n",
    "      \n",
    "          \n",
    "                \n",
    "        \n",
    "        print \"company:- \" + company[0]\n",
    "        print \"words_after_ending_loc\"\n",
    "        print words_after_ending_loc\n",
    "        print \"words_before_starting_loc\"\n",
    "        print words_before_starting_loc '''\n",
    "        \n",
    "        company.extend([hasCompanyid, hasCompanyNameFirst, hasNameSubstring, isInFilename, has_based, has_apostrophe_s, has_shares, has_desig, has_said])\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = os.path.dirname(os.getcwd())\n",
    "data_directory = os.path.join(base_directory,\"Dataset\",\"Testing_1\")\n",
    "file_list = os.listdir(data_directory)\n",
    "possible_words=[]\n",
    "processed_list=[]\n",
    "count=0\n",
    "#doc_id = 101\n",
    "for file in file_list:\n",
    "    relevant_filename = file[4:-4]\n",
    "    doc_id = file[0:3]\n",
    "    #print(doc_id)\n",
    "    #if(int(doc_id)!=95):\n",
    "    #    continue    \n",
    "    possible_words = getAllPossibleWords(os.path.join(data_directory,file))     \n",
    "    \n",
    "    #start from here .. processessed list has the structure[[word,doc_id,word_count_start,word_count_end,label]]\n",
    "    ''''processed_list.append(preprocessing(possible_words,doc_id))\n",
    "    writeToCsv(processed_list)'''\n",
    "    processed_list_for_doc = preprocessing(possible_words,doc_id)\n",
    "    #print processed_list_for_doc\n",
    "    generateFeatures(relevant_filename, processed_list_for_doc,possible_words)\n",
    "    processed_list.extend(processed_list_for_doc)\n",
    "    #writeToCsv(processed_list) \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallo\n"
     ]
    }
   ],
   "source": [
    "ngram = ('<markup>Gazprom', 'Lukoil</markup>')\n",
    "if(len(ngram)>1 and '<markup>' in ngram[0] and '</markup>' in ngram[len(ngram)-1] and\n",
    "              all((word[0].isupper() or \"markup\" in word) for word in ngram) and \n",
    "              [\"<markup>\" in word for word in ngram].count(True)==1):\n",
    "    print \"hallo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"markup\" in word for word in ngram].count(True)==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n"
     ]
    }
   ],
   "source": [
    "import pycountry\n",
    "country_names = []\n",
    "for country in pycountry.countries:\n",
    "    country_names.append(country.name)\n",
    "    if hasattr(country,'common_name'):\n",
    "        country_names.append(country.common_name)\n",
    "    if hasattr(country, 'official_name'):\n",
    "        country_names.append(country.official_name)\n",
    "\n",
    "country_names.append(\"America\")\n",
    "country_names.append(\"Latin America\")\n",
    "country_names.append(\"Korea\")\n",
    "country_names.append(\"South Korea\")\n",
    "country_names.append(\"North Korea\")\n",
    "country_names.append(\"U.S.\")\n",
    "country_names.append(\"Russia\")\n",
    "country_names.append(\"Siberia\")\n",
    "\n",
    "if \"Russia\" in country_names:\n",
    "    print \"YES\"\n",
    "    \n",
    "continent_list = ['Asia', 'Africa', 'South America', 'North America', 'Antarctica', 'Europe', 'Australia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n"
     ]
    }
   ],
   "source": [
    "cities = []\n",
    "with open('world-cities.csv', 'r') as inp:\n",
    "        for row in csv.reader(inp):\n",
    "            if(row and row[3] != \"\" and row[0]!=\"name\"):\n",
    "                cities.append(row[0])\n",
    "\n",
    "cities.append('Bombay')\n",
    "cities.append('New York')\n",
    "cities.append('Montreal')\n",
    "cities.append('Frankfurt')\n",
    "cities.append('Zurich')\n",
    "if \"Bedford\" in cities:\n",
    "    print \"YES\"\n",
    "\n",
    "us_states = open('us-states.txt').read().split('\\n')\n",
    "ethni_list = open('ethnicities.txt').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chief executive officer', 'chief financial officer', 'co-chief executive', 'chief investment officer', 'analyst', 'ceo', 'cio', 'cto', 'cfo', 'manager', 'director', 'president', 'vice-president', 'vice president', 'chief', 'executive', 'officer', 'staff', 'secretary', 'attorney', 'jr', 'sr', 'officials', 'officers', 'analysts', 'managers', 'directors', 'co-chief', 'co chief', 'commissioner', 'minister', 'magistrate', 'judge', 'chairman']\n"
     ]
    }
   ],
   "source": [
    "desig = open('designations.txt').read().split('\\n')\n",
    "desig_lower = [x.lower() for x in desig]\n",
    "\n",
    "print desig_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3595\n"
     ]
    }
   ],
   "source": [
    "pruned_pos = open('pruned_pos.txt').read().split('\\n')\n",
    "stock_x = open('stock-exchange-list.txt').read().split('\\n')\n",
    "unigram_prune = open('unigram-prune-neg.txt').read().split('\\n')\n",
    "special_char = [';', ',', ':', '?']\n",
    "prune = open('pruning-neg.txt').read().split('\\n') \n",
    "prune_lower = [x.lower() for x in prune]\n",
    "suffixes = open('company-suffixes.txt').read().split('\\n')\n",
    "prefixes = open('company-prefixes.txt').read().split('\\n')\n",
    "rows = 0\n",
    "print len(processed_list)\n",
    "for doc in processed_list:\n",
    "    rows += len(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ACE', \"Sam's Club\", 'ICBC', 'KKR', 'CSN', 'Tata', 'Wheeling', \"River's Edge Pharmaceuticals LLC\", \"River's Edge\", 'RWE', 'Fremont', 'News Corp', 'GM', 'Liberty', 'Financial Services Authority', 'FSA', 'NZOG', 'Norilsk', 'NTP', 'MPC', 'PPR', 'THQ', 'Nokia', 'RCS', 'Intercontinental Exchange Inc', 'EON', 'RHB', 'WCM', 'Macclesfield', 'Carrefour', 'KPI', 'TXU', 'ECI', 'BP', 'TNK-BP', 'Imperial', 'Zug', 'TDF', 'TMK', \"Ivernia Inc's\", 'SAIC', 'GE', 'PICC', 'Saku', 'Winterthur', 'ZKB', \"Rupert Murdoch's News Corp\", 'PIK', 'VTB', 'UBS', 'IGT', 'BHP', 'BBH', 'China Banking Regulatory Commission', 'Compton', 'Centennial', 'NPD', 'HSBC', \"Victoria's Secret\", 'MAN', 'MBIA', 'Freeport', 'MGIC', 'PMI', 'Stillwater', \"Dr Reddy's Laboratories Ltd\", \"Royal Mail's\", 'Cia Siderurgica Nacional SA', 'Cia Siderurgica Nacional', 'Vector Hospitality Plc', 'Vector', 'Victory', 'Bedford', 'Equity Office Properties']\n",
      "1664\n",
      "1931\n",
      "3595\n"
     ]
    }
   ],
   "source": [
    "print pruned_pos\n",
    "new_proc_list = []\n",
    "pruned_list = []\n",
    "\n",
    "for inst in processed_list:\n",
    "    if inst[0] not in pruned_pos and not inst[0].endswith(tuple(suffixes)) and not inst[0].startswith(tuple(prefixes)): \n",
    "        if inst[0] in country_names or inst[0] in continent_list or inst[0] in cities or inst[0] in stock_x or inst[0] in us_states:\n",
    "            pruned_list.append(inst)\n",
    "        elif \"'s\" in inst[0]:\n",
    "            pruned_list.append(inst)\n",
    "        elif inst[0].endswith('n') and inst[0][:-1] in country_names or str in continent_list:\n",
    "            pruned_list.append(inst)\n",
    "        elif \"based\" in inst[0]:\n",
    "            pruned_list.append(inst)\n",
    "        elif any(d in inst[0].lower().split() for d in desig_lower):\n",
    "            pruned_list.append(inst)\n",
    "        elif any(p in inst[0].lower().split() for p in prune_lower):\n",
    "            pruned_list.append(inst)\n",
    "        elif '(' in inst[0] or ')' in inst[0]:\n",
    "            pruned_list.append(inst)\n",
    "        elif any(c in inst[0] for c in special_char):\n",
    "            pruned_list.append(inst)\n",
    "        elif inst[0].startswith(\"A \") or inst[0].startswith(\"An \") or inst[0].startswith(\"And \") or inst[0].startswith(\"This\"):\n",
    "             pruned_list.append(inst)\n",
    "        elif inst[0].endswith(\"And\") or inst[0].endswith(\"This\"):\n",
    "             pruned_list.append(inst)\n",
    "        elif len(inst[0].split()) == 1:\n",
    "            if len(inst[0]) == 1:\n",
    "                pruned_list.append(inst)\n",
    "            elif inst[0] in unigram_prune or inst[0] in ethni_list:\n",
    "                pruned_list.append(inst)\n",
    "            elif inst[0] == inst[0].upper():\n",
    "                pruned_list.append(inst)\n",
    "            elif inst[0] in suffixes or inst[0] in prefixes:\n",
    "                pruned_list.append(inst)\n",
    "            else:\n",
    "                new_proc_list.append(inst)\n",
    "        else:\n",
    "            new_proc_list.append(inst)\n",
    "    else:\n",
    "        new_proc_list.append(inst)\n",
    "            \n",
    "                \n",
    "print len(pruned_list)\n",
    "for eg in pruned_list:\n",
    "    if eg[4] == 1:\n",
    "        print eg[0] + \" \" + str(eg[1])\n",
    "#writeToCsv(processed_list)\n",
    "print len(new_proc_list)\n",
    "print len(pruned_list) + len(new_proc_list)\n",
    "#print pruned_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_pruned_sr.csv\", \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    for val in new_proc_list:\n",
    "        writer.writerow(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
