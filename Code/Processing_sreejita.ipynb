{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_value = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(input_list, n):    \n",
    "    return zip(*[input_list[i:] for i in range(n)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllPossibleWords(file_path): \n",
    "    possible_words=[]\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            for word in line.split():\n",
    "                possible_words.append(word)     \n",
    "   # possible_words = map(lambda x : unidecode.unidecode(x),possible_words)\n",
    "    for c in ['`',\"[\",\"]\",\"(\",\")\",\"\\\"\",\".\",\"''\",\",\"]:\n",
    "        possible_words = map(lambda x: x.replace(c,\"\"),possible_words)\n",
    "        \n",
    "    possible_words = map(lambda x: x.replace('&','And'),possible_words)\n",
    "    #possible_words = map(lambda x: x.replace('of','Of'),possible_words)\n",
    "    possible_words=filter(lambda x:len(x)>0,possible_words)    \n",
    "    \n",
    "    return possible_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListFromFiles(filename):\n",
    "    file_path = os.path.join(os.getcwd(),filename)\n",
    "    info_list = []\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            for word in line.split():\n",
    "                info_list.append(word)\n",
    "    return info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToCsv(processed_list):\n",
    "    with open(\"output-sreejita.csv\",'wb') as resultFile:\n",
    "        wr = csv.writer(resultFile, dialect='excel')\n",
    "        #for row in processed_list:\n",
    "        wr.writerows(processed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def preprocessing(possible_words,doc_id):    \\n    suffix_list = getListFromFiles(\"company-suffixes.txt\")\\n    prefix_list = getListFromFiles(\"company-prefixes.txt\")    \\n    common_words = getListFromFiles(\"common-words.txt\")\\n    preprocessed_list =[]    \\n    marked_list=[]\\n    word_number=len(possible_words)\\n    for i in range(5,0,-1):\\n        word_count=0\\n        ngram_list = get_ngrams(possible_words,i)\\n        for ngram in ngram_list:            \\n            #print ngram   \\n            \\n            #if markup is both at the beginning or at the end of the word group accept and label as 1\\n            if(len(ngram)>1 and \\'<markup>\\' in ngram[0] and \\'</markup>\\' in ngram[len(ngram)-1] and\\n              all((word[0].isupper() or \"markup\" in word) for word in ngram) and \\n              [\"<markup>\" in word for word in ngram].count(True)==1):\\n                preprocessed_list.append([\\' \\'.join(ngram),doc_id,word_count,word_count+i-1,1])\\n            \\n            #get all instances of format <markup>Microsoft</markup>\\n            if(len(ngram)==1 and \\'</markup>\\' in ngram[0] and \\'<markup>\\' in ngram[0]):\\n                preprocessed_list.append([\\' \\'.join(ngram),doc_id,word_count,word_count+i-1,1])            \\n                    \\n            else:\\n                #prune away all n grams with non uppercase first character\\n                if(all(word[0].isupper() and \"markup\" not in word for word in ngram)):       \\n                    \\n                    # prune common words\\n                    if(any(word not in common_words for word in ngram)):                        \\n                        preprocessed_list.append([\\' \\'.join(ngram),doc_id,word_count,word_count+i-1,0])\\n\\n                    #prune unigrams \\n                    elif(i==1 and word_number> word_count+1):                        \\n                        \\n                        #add the unigram to the list if the next word begins with lower case\\n                        if(possible_words[word_count+1][0].islower()):\\n                            preprocessed_list.append([\\' \\'.join(ngram),doc_id,word_count,word_count+i-1,0])  \\n            word_count = word_count+1\\n    return preprocessed_list   '"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def preprocessing(possible_words,doc_id):    \n",
    "    suffix_list = getListFromFiles(\"company-suffixes.txt\")\n",
    "    prefix_list = getListFromFiles(\"company-prefixes.txt\")    \n",
    "    common_words = getListFromFiles(\"common-words.txt\")\n",
    "    preprocessed_list =[]    \n",
    "    marked_list=[]\n",
    "    word_number=len(possible_words)\n",
    "    for i in range(5,0,-1):\n",
    "        word_count=0\n",
    "        ngram_list = get_ngrams(possible_words,i)\n",
    "        for ngram in ngram_list:            \n",
    "            #print ngram   \n",
    "            \n",
    "            #if markup is both at the beginning or at the end of the word group accept and label as 1\n",
    "            if(len(ngram)>1 and '<markup>' in ngram[0] and '</markup>' in ngram[len(ngram)-1] and\n",
    "              all((word[0].isupper() or \"markup\" in word) for word in ngram) and \n",
    "              [\"<markup>\" in word for word in ngram].count(True)==1):\n",
    "                preprocessed_list.append([' '.join(ngram),doc_id,word_count,word_count+i-1,1])\n",
    "            \n",
    "            #get all instances of format <markup>Microsoft</markup>\n",
    "            if(len(ngram)==1 and '</markup>' in ngram[0] and '<markup>' in ngram[0]):\n",
    "                preprocessed_list.append([' '.join(ngram),doc_id,word_count,word_count+i-1,1])            \n",
    "                    \n",
    "            else:\n",
    "                #prune away all n grams with non uppercase first character\n",
    "                if(all(word[0].isupper() and \"markup\" not in word for word in ngram)):       \n",
    "                    \n",
    "                    # prune common words\n",
    "                    if(any(word not in common_words for word in ngram)):                        \n",
    "                        preprocessed_list.append([' '.join(ngram),doc_id,word_count,word_count+i-1,0])\n",
    "\n",
    "                    #prune unigrams \n",
    "                    elif(i==1 and word_number> word_count+1):                        \n",
    "                        \n",
    "                        #add the unigram to the list if the next word begins with lower case\n",
    "                        if(possible_words[word_count+1][0].islower()):\n",
    "                            preprocessed_list.append([' '.join(ngram),doc_id,word_count,word_count+i-1,0])  \n",
    "            word_count = word_count+1\n",
    "    return preprocessed_list   ''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(possible_words,doc_id):    \n",
    "    suffix_list = getListFromFiles(\"company-suffixes.txt\")\n",
    "    prefix_list = getListFromFiles(\"company-prefixes.txt\")    \n",
    "    common_words = getListFromFiles(\"common-words.txt\")\n",
    "    preprocessed_list =[]    \n",
    "    marked_list=[]\n",
    "    word_number=len(possible_words)\n",
    "    for i in range(5,0,-1):\n",
    "        word_count=0\n",
    "        ngram_list = get_ngrams(possible_words,i)\n",
    "        for ngram in ngram_list:            \n",
    "            #print ngram   \n",
    "            \n",
    "            #if markup is both at the beginning or at the end of the word group accept and label as 1\n",
    "            if(len(ngram)>1 and '<markup>' in ngram[0] and '</markup>' in ngram[len(ngram)-1] and\n",
    "              all((word[0].isupper() or \"markup\" in word) for word in ngram) and \n",
    "              [\"<markup>\" in word for word in ngram].count(True)==1): \n",
    "                company_tuple = ngram\n",
    "                for string in [\"<markup>\",\"</markup>\"]:\n",
    "                    company_tuple = map(lambda x : x.replace(string,\"\"),company_tuple)                \n",
    "                preprocessed_list.append([' '.join(company_tuple),doc_id,word_count,word_count+i-1,1])\n",
    "            \n",
    "            #get all instances of format <markup>Microsoft</markup>\n",
    "            elif(len(ngram)==1 and '</markup>' in ngram[0] and '<markup>' in ngram[0]):\n",
    "                \n",
    "                company_tuple = ngram\n",
    "                for string in [\"<markup>\",\"</markup>\"]:\n",
    "                    company_tuple = map(lambda x : x.replace(string,\"\"),company_tuple) \n",
    "                \n",
    "                preprocessed_list.append([' '.join(company_tuple),doc_id,word_count,word_count+i-1,1])            \n",
    "                    \n",
    "            else:\n",
    "                #prune away all n grams with non uppercase first character\n",
    "                if(all(word[0].isupper() and \"markup\" not in word for word in ngram)):  \n",
    "                    \n",
    "                    #prune unigrams \n",
    "                    if(i==1):                        \n",
    "                        if(ngram[0] not in suffix_list):\n",
    "                            if(word_number> word_count+1 and ngram[0] not in common_words):\n",
    "                                #add the unigram to the list if the next word begins with lower case\n",
    "                                if(possible_words[word_count+1][0].islower()):\n",
    "                                    preprocessed_list.append([' '.join(ngram),doc_id,word_count,word_count+i-1,0])\n",
    "                    \n",
    "                    #prune common words\n",
    "                    elif(any(word not in common_words for word in ngram)):                        \n",
    "                        preprocessed_list.append([' '.join(ngram),doc_id,word_count,word_count+i-1,0])                   \n",
    "\n",
    "                    \n",
    "                     \n",
    "            word_count = word_count+1\n",
    "    return preprocessed_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFeatures(processed_list,possible_words):\n",
    "    \n",
    "    company_suffixes = getListFromFiles(\"company-suffixes.txt\")\n",
    "    company_prefixes = getListFromFiles(\"company-prefixes.txt\")    \n",
    "    company_list=[]\n",
    "    \n",
    "    for company in processed_list:\n",
    "    \n",
    "        #Feature 1 : name has any prefix or suffix of a company \n",
    "        #if company name has any one of the suffixes or prefixes set it to true \n",
    "        hasCompanyid=0        \n",
    "        if(company[0].split()[-1] in company_suffixes or company[0].split()[0] in company_prefixes):\n",
    "            hasCompanyid = 1\n",
    "            company_list.append(company[0])            \n",
    "        #company.append(hasCompanyid)\n",
    "        \n",
    "        \n",
    "        #has Company name at the beginning ?\n",
    "        hasCompanyNameFirst = 0\n",
    "        l1= [x for x in company[0].split() if x not in company_prefixes]\n",
    "        l2=[]\n",
    "        for comp in company_list:    \n",
    "            if(comp.split()[0] in company_prefixes):\n",
    "                l2.append(\" \".join(comp.split()[1:]))\n",
    "            else:\n",
    "                l2.append(comp)\n",
    "        if(any(word in l2 for word in l1)):\n",
    "            hasCompanyNameFirst = 1\n",
    "        elif(any(company[0].split()[0]== word.split()[0] for word in company_list)):\n",
    "            hasCompanyNameFirst = 1 \n",
    "        \n",
    "            \n",
    "        \n",
    "        #has Company name as a substring ?\n",
    "        hasNameSubstring=0\n",
    "        for comp in company_list:\n",
    "            l1 = company[0].split()\n",
    "            l2 = comp.split()\n",
    "            if(len(set(l1)&set(l2)) > 0 ):\n",
    "                hasNameSubstring=1       \n",
    "       \n",
    "        company.extend([hasCompanyid,hasCompanyNameFirst,hasNameSubstring])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = os.path.dirname(os.getcwd())\n",
    "data_directory = os.path.join(base_directory,\"Dataset\",\"Training\")\n",
    "file_list = os.listdir(data_directory)\n",
    "possible_words=[]\n",
    "processed_list=[]\n",
    "count=0\n",
    "#doc_id = 101\n",
    "for file in file_list:\n",
    "    doc_id = file[0:3]\n",
    "    #print(doc_id)\n",
    "    #if(int(doc_id)!=95):\n",
    "    #    continue    \n",
    "    possible_words = getAllPossibleWords(os.path.join(data_directory,file))     \n",
    "    \n",
    "    #start from here .. processessed list has the structure[[word,doc_id,word_count_start,word_count_end,label]]\n",
    "    ''''processed_list.append(preprocessing(possible_words,doc_id))\n",
    "    writeToCsv(processed_list)'''\n",
    "    processed_list_for_doc = preprocessing(possible_words,doc_id)\n",
    "    generateFeatures(processed_list_for_doc,possible_words)\n",
    "    processed_list.extend(processed_list_for_doc)\n",
    "    #writeToCsv(processed_list) \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallo\n"
     ]
    }
   ],
   "source": [
    "ngram = ('<markup>Gazprom', 'Lukoil</markup>')\n",
    "if(len(ngram)>1 and '<markup>' in ngram[0] and '</markup>' in ngram[len(ngram)-1] and\n",
    "              all((word[0].isupper() or \"markup\" in word) for word in ngram) and \n",
    "              [\"<markup>\" in word for word in ngram].count(True)==1):\n",
    "    print \"hallo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"markup\" in word for word in ngram].count(True)==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n"
     ]
    }
   ],
   "source": [
    "import pycountry\n",
    "country_names = []\n",
    "for country in pycountry.countries:\n",
    "    country_names.append(country.name)\n",
    "    if hasattr(country,'common_name'):\n",
    "        country_names.append(country.common_name)\n",
    "    if hasattr(country, 'official_name'):\n",
    "        country_names.append(country.official_name)\n",
    "\n",
    "country_names.append(\"America\")\n",
    "country_names.append(\"Korea\")\n",
    "country_names.append(\"South Korea\")\n",
    "country_names.append(\"North Korea\")\n",
    "country_names.append(\"U.S.\")\n",
    "country_names.append(\"Russia\")\n",
    "\n",
    "if \"Russia\" in country_names:\n",
    "    print \"YES\"\n",
    "    \n",
    "continent_list = ['Asia', 'Africa', 'South America', 'North America', 'Antarctica', 'Europe', 'Australia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n"
     ]
    }
   ],
   "source": [
    "cities = []\n",
    "with open('world-cities.csv', 'r') as inp:\n",
    "        for row in csv.reader(inp):\n",
    "            if(row and row[3] != \"\" and row[0]!=\"name\"):\n",
    "                cities.append(row[0])\n",
    "\n",
    "cities.append('Bombay')\n",
    "cities.append('New York')\n",
    "if \"Compton\" in cities:\n",
    "    print \"YES\"\n",
    "\n",
    "us_states = open('us-states.txt').read().split('\\n')\n",
    "ethni_list = open('ethnicities.txt').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "desig = open('designations.txt').read().split('\\n')\n",
    "desig_lower = [x.lower() for x in desig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8115\n"
     ]
    }
   ],
   "source": [
    "pruned_pos = open('pruned_pos.txt').read().split('\\n')\n",
    "stock_x = open('stock-exchange-list.txt').read().split('\\n')\n",
    "unigram_prune = open('unigram-prune-neg.txt').read().split('\\n')\n",
    "special_char = [';', ',', ':', '?']\n",
    "prune = open('pruning-neg.txt').read().split('\\n') \n",
    "prune_lower = [x.lower() for x in prune]\n",
    "suffixes = open('company-suffixes.txt').read().split('\\n')\n",
    "prefixes = open('company-prefixes.txt').read().split('\\n')\n",
    "rows = 0\n",
    "print len(processed_list)\n",
    "for doc in processed_list:\n",
    "    rows += len(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ACE', \"Sam's Club\", 'ICBC', 'KKR', 'CSN', 'Tata', 'Wheeling', \"River's Edge Pharmaceuticals LLC\", \"River's Edge\", 'RWE', 'Fremont', 'News Corp', 'GM', 'Liberty', 'Financial Services Authority', 'FSA', 'NZOG', 'Norilsk', 'NTP', 'MPC', 'PPR', 'THQ', 'Nokia', 'RCS', 'Intercontinental Exchange Inc', 'EON', 'RHB', 'WCM', 'Macclesfield', 'Carrefour', 'KPI', 'TXU', 'ECI', 'BP', 'TNK-BP', 'Imperial', 'Zug', 'TDF', 'TMK', \"Ivernia Inc's\", 'SAIC', 'GE', 'PICC', 'Saku', 'Winterthur', 'ZKB', \"Rupert Murdoch's News Corp\", 'PIK', 'VTB', 'UBS', 'IGT', 'BHP', 'BBH', 'China Banking Regulatory Commission', 'Compton', 'Centennial', 'NPD', 'HSBC', \"Victoria's Secret\", 'MAN', 'MBIA', 'Freeport', 'MGIC', 'PMI', 'Stillwater', \"Dr Reddy's Laboratories Ltd\", \"Royal Mail's\", '']\n",
      "2914\n",
      "5201\n",
      "8115\n"
     ]
    }
   ],
   "source": [
    "print pruned_pos\n",
    "new_proc_list = []\n",
    "pruned_list = []\n",
    "\n",
    "for inst in processed_list:\n",
    "    if inst[0] not in pruned_pos: \n",
    "        if inst[0] in country_names or inst[0] in continent_list or inst[0] in cities or inst[0] in stock_x or inst[0] in us_states:\n",
    "            pruned_list.append(inst)\n",
    "        elif \"'s\" in inst[0]:\n",
    "            pruned_list.append(inst)\n",
    "        elif inst[0].endswith('n') and inst[0][:-1] in country_names or str in continent_list:\n",
    "            pruned_list.append(inst)\n",
    "        elif \"based\" in inst[0]:\n",
    "            pruned_list.append(inst)\n",
    "        elif any(d in inst[0].lower().split() for d in desig_lower):\n",
    "            pruned_list.append(inst)\n",
    "        elif any(p in inst[0].lower().split() for p in prune_lower):\n",
    "            pruned_list.append(inst)\n",
    "        elif '(' in inst[0] or ')' in inst[0]:\n",
    "            pruned_list.append(inst)\n",
    "        elif any(c in inst[0] for c in special_char):\n",
    "            pruned_list.append(inst)\n",
    "        elif len(inst[0].split()) == 1:\n",
    "            if len(inst[0]) == 1:\n",
    "                pruned_list.append(inst)\n",
    "            elif inst[0] in unigram_prune or inst[0] in ethni_list:\n",
    "                pruned_list.append(inst)\n",
    "            elif inst[0] == inst[0].upper():\n",
    "                pruned_list.append(inst)\n",
    "            elif inst[0] in suffixes or inst[0] in prefixes:\n",
    "                pruned_list.append(inst)\n",
    "            else:\n",
    "                new_proc_list.append(inst)\n",
    "        else:\n",
    "            new_proc_list.append(inst)\n",
    "    else:\n",
    "        new_proc_list.append(inst)\n",
    "            \n",
    "                \n",
    "print len(pruned_list)\n",
    "for eg in pruned_list:\n",
    "    if eg[4] == 1:\n",
    "        print \"HOLY MOLY!\"\n",
    "#writeToCsv(processed_list)\n",
    "print len(new_proc_list)\n",
    "print len(pruned_list) + len(new_proc_list)\n",
    "#print pruned_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thefile = open(\\'pruned_pos.txt\\', \\'w\\')\\nfor item in pruned_pos:\\n    #print item\\n    thefile.write(\"%s\\n\" % item)\\nlen(pruned_pos)\\nprint pruned_list'"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''thefile = open('pruned_pos.txt', 'w')\n",
    "for item in pruned_pos:\n",
    "    #print item\n",
    "    thefile.write(\"%s\\n\" % item)\n",
    "len(pruned_pos)\n",
    "print pruned_list'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_pruned.csv\", \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    for val in new_proc_list:\n",
    "        writer.writerow(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
